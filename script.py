import requests
import logging
import subprocess
from concurrent.futures import ThreadPoolExecutor
import time
from collections import defaultdict
import os

# é…ç½®æ—¥å¿—è®°å½•
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def check_url_validity(url, max_retries=3):
    for attempt in range(max_retries):
        try:
            response = requests.head(url, timeout=10)
            response.raise_for_status()
            return True
        except requests.exceptions.RequestException as e:
            if attempt == max_retries - 1:
                logging.error(f"URL {url} failed after {max_retries} attempts: {e}")
                return False
            time.sleep(1)  # é‡è¯•å‰ç­‰å¾…

def fetch_content(url):
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        return response.content.decode('utf-8-sig')
    except:
        return None

def filter_content(content):
    if content is None:
        return []
    keywords = ["ãŠ™VIPæµ‹è¯•", "å…³æ³¨å…¬ä¼—å·", "å¤©å¾®ç§‘æŠ€", "è·å–æµ‹è¯•å¯†ç ", "æ›´æ–°æ—¶é—´", "â™¥èšç©ç›’å­", "ğŸŒ¹é˜²å¤±è”","ğŸ“¡  æ›´æ–°æ—¥æœŸ","ğŸ‘‰",]
    return [line for line in content.splitlines() if 'ipv6' not in line.lower() and not any(keyword in line for keyword in keywords)]

def check_stream_quality(url):
    """æ£€æŸ¥æµçš„è´¨é‡å¹¶è¿”å›ä¸€ä¸ªè´¨é‡åˆ†æ•°"""
    try:
        command = ['ffmpeg', '-i', url, '-t', '10', '-f', 'null', '-']
        start_time = time.time()
        result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, timeout=20)
        duration = time.time() - start_time
        
        if result.returncode == 0:
            # åŸºç¡€åˆ†æ•° 100
            score = 100
            # å“åº”æ—¶é—´è¶ŠçŸ­è¶Šå¥½ï¼Œæ¯è¶…è¿‡1ç§’æ‰£1åˆ†ï¼Œæœ€å¤šæ‰£20åˆ†
            score -= min(20, int(duration))
            # æ£€æŸ¥ffmpegè¾“å‡ºä¸­çš„é”™è¯¯å’Œè­¦å‘Šæ•°é‡
            stderr = result.stderr.decode('utf-8')
            errors = stderr.count('Error') * 5  # æ¯ä¸ªé”™è¯¯æ‰£5åˆ†
            warnings = stderr.count('Warning') * 2  # æ¯ä¸ªè­¦å‘Šæ‰£2åˆ†
            score -= (errors + warnings)
            return max(0, score)  # ç¡®ä¿åˆ†æ•°ä¸å°äº0
        return 0
    except:
        return 0

def fetch_and_filter(urls):
    filtered_lines = []
    
    # è°ƒæ•´URLè·å–çš„å¹¶å‘æ•°ï¼Œè®¾ç½®ä¸ºCPUæ ¸å¿ƒæ•°çš„2å€
    max_fetch_workers = min(32, os.cpu_count() * 2 or 4)
    with ThreadPoolExecutor(max_workers=max_fetch_workers) as executor:
        valid_urls = [url for url in urls if check_url_validity(url)]
        results = list(executor.map(fetch_content, valid_urls))
    
    for content in results:
        filtered_lines.extend(filter_content(content))
    
    # ç”¨äºå­˜å‚¨æŒ‰é¢‘é“åˆ†ç»„çš„URL
    channel_groups = defaultdict(list)
    current_genre = "æœªåˆ†ç±»"  # é»˜è®¤åˆ†ç±»
    
    # é¦–å…ˆæŒ‰é¢‘é“åˆ†ç»„
    for line in filtered_lines:
        line = line.strip()
        if not line:  # è·³è¿‡ç©ºè¡Œ
            continue
            
        if line.startswith('#genre#'):
            current_genre = line
            channel_groups[current_genre].append(line)
        elif line.startswith('http'):
            # ä¿®æ”¹URLå¤„ç†é€»è¾‘
            parts = line.split(',')
            if len(parts) >= 2:
                url = parts[0]
                channel_name = ','.join(parts[1:])  # å¤„ç†é¢‘é“åä¸­å¯èƒ½åŒ…å«é€—å·çš„æƒ…å†µ
                channel_groups[current_genre].append(line)
            else:
                # å¦‚æœURLæ²¡æœ‰é¢‘é“åï¼Œå°†å…¶æ·»åŠ åˆ°å½“å‰åˆ†ç±»
                channel_groups[current_genre].append(line)
        else:
            channel_groups[current_genre].append(line)
    
    # è°ƒæ•´æµåª’ä½“è´¨é‡æ£€æµ‹çš„å¹¶å‘æ•°
    max_stream_workers = min(5, os.cpu_count() or 2)
    valid_lines = []
    
    # æ·»åŠ è°ƒè¯•æ—¥å¿—
    logging.info(f"å¼€å§‹å¤„ç†é¢‘é“ç»„ï¼Œå…± {len(channel_groups)} ä¸ªåˆ†ç»„")
    
    with ThreadPoolExecutor(max_workers=max_stream_workers) as executor:
        for genre, urls in channel_groups.items():
            logging.info(f"å¤„ç†åˆ†ç»„: {genre}, åŒ…å« {len(urls)} ä¸ªURL")
            
            if genre.startswith('#genre#'):
                valid_lines.append(genre)
                continue
            
            # æ‰¹é‡æäº¤æ£€æµ‹ä»»åŠ¡
            url_scores = []
            batch_size = max_stream_workers
            
            # åªå¯¹HTTPé“¾æ¥è¿›è¡Œè´¨é‡æ£€æµ‹
            http_urls = [url for url in urls if url.startswith('http')]
            
            for i in range(0, len(http_urls), batch_size):
                batch_urls = http_urls[i:i + batch_size]
                batch_scores = []
                
                for url_line in batch_urls:
                    url = url_line.split(',')[0]
                    try:
                        score = executor.submit(check_stream_quality, url)
                        batch_scores.append((url_line, score))
                    except Exception as e:
                        logging.error(f"Error submitting quality check for {url}: {e}")
                
                # ç­‰å¾…å½“å‰æ‰¹æ¬¡å®Œæˆ
                for url_line, score in batch_scores:
                    try:
                        quality_score = score.result(timeout=30)
                        if quality_score > 0:
                            url_scores.append((url_line, quality_score))
                            logging.info(f"URLè´¨é‡åˆ†æ•°: {url_line} -> {quality_score}")
                    except Exception as e:
                        logging.error(f"Error checking quality for {url_line}: {e}")
            
            # å¯¹å½“å‰é¢‘é“çš„æ‰€æœ‰URLè¿›è¡Œæ’åº
            sorted_urls = sorted(url_scores, key=lambda x: x[1], reverse=True)
            valid_lines.extend(url_line for url_line, score in sorted_urls)
    
    # ç¡®ä¿ç»“æœä¸ä¸ºç©º
    if not valid_lines:
        logging.warning("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ç›´æ’­æº")
        return
    
    # ä¿å­˜ç»“æœ
    with open('live_ipv4.txt', 'w', encoding='utf-8') as file:
        file.write('\n'.join(valid_lines))
    
    # éªŒè¯æ–‡ä»¶æ˜¯å¦å†™å…¥æˆåŠŸ
    if os.path.exists('live_ipv4.txt'):
        with open('live_ipv4.txt', 'r', encoding='utf-8') as file:
            content = file.read()
            logging.info(f"æ–‡ä»¶å†™å…¥æˆåŠŸï¼Œå…± {len(content.splitlines())} è¡Œ")
    else:
        logging.error("æ–‡ä»¶å†™å…¥å¤±è´¥")

if __name__ == "__main__":
    urls = [
        'https://raw.githubusercontent.com/leiyou-li/IPTV4/refs/heads/main/live.txt',
        'https://raw.githubusercontent.com/kimwang1978/collect-tv-txt/main/merged_output.txt',
        'http://xhztv.top/zbc.txt',
        'http://ww.weidonglong.com/dsj.txt',
        'https://tv.youdu.fan:666/live/',
        'https://live.zhoujie218.top/tv/iptv6.txt',
        'http://tipu.xjqxz.top/live1213.txt',
        'https://tv.iill.top/m3u/Live',
        'http://www.lyyytv.cn/yt/zhibo/1.txt',
        'http://live.nctv.top/x.txt',
        'http://www.lyyytv.cn/yt/zhibo/1.txt',
        'https://github.moeyy.xyz/https://raw.githubusercontent.com/Ftindy/IPTV-URL/main/huyayqk.m3u',
        'https://ghp.ci/raw.githubusercontent.com/MemoryCollection/IPTV/refs/heads/main/itvlist.m3u',
        'https://live.fanmingming.com/tv/m3u/ipv6.m3u'
    ]
    fetch_and_filter(urls)